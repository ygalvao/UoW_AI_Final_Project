{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id='Top'></a>\n<center>\n    <h1><b>Using Machine Learning to Analyze the 2nd Round of the 2022 Brazilian Presidential Election</b></h1>\n<h3>Author: Yuri Henrique Galvao - Student # 3151850</h3>\n</center>\n\n---\nThis is the Final Project for the Artificial Intelligence Diploma program of The University of Winnipeg - Professional, Applied and Continuing Education (PACE). The idea of this project is to extract the available data generated by the Electronic Voting Machines (EVM) that were used in the 2nd Round of the 2022 Brazilian Presidential Election, clean it, analyze it, and then use clustering models to find data patterns - especially hidden or non-intuitive patterns - and anomalies.\n\nFor these clustering and anomaly-detection tasks, I will use the following three unsupervised clustering algorithms, which will be presented and compared: K-Means, Density-Based Spatial Clustering of Applications with Noise (DBSCAN), and Self-Organizing Maps (SOMs).\n\nThis notebook is divided into 6 main sections. At the end of the notebook, you will find also the references for the content that I used during the preparation of this notebook. In order to increase readability, some code is hidden (so, please unhide them if you want).\n\n### SECTIONS:  \n1. [Introduction](#Intro)<br>  \n2. [Data Ingestion](#Data_ingestion)<br>  \n3. [Exploratory Data Analysis](#Exploratory_Data_Analysis)<br>\n4. [Clustering](#Clustering)<br>\n    4.1 [K-Means](#K-Means)<br>\n    4.2 [DBSCAN](#DBSCAN)<br>\n    4.3 [SOM](#SOM)<br>\n5. [Conclusions](#Conclusions)<br>\n6. [References](#References)<br>\n\n<a id='Intro'></a>\n## 1. Introduction  <a href='#Top' style=\"text-decoration: none;\">^</a>\n### 1.1. Context\n\nIn October of the current year (2022) Brazil had the fiercest presidential election of the past three\ndecades. During the first round of federal elections, more than 10 candidates ran for President of the Federative Republic of Brazil, but all Brazilians knew that only two candidates would have real chances of going to the second round: Luis Inácio “Lula” da Silva and Jair Messias Bolsonaro.\n\nAs everyone expected, the presidential elections went to the second round with Lula and\nBolsonaro, and Brazil had a very controversial election in which Lula won with 50.9% of the valid votes against 49.1% of the valid votes that Bolsonaro received.\n\nThose election were controversial because, among with other issues, Brazil uses an 100%\nelectronic voting system, which is based on a digital electronic voting machine (EVM) and in a digital voting processing system which is considered a black-box system.\n\n### 1.2. Project Idea\n\nIn an attempt to make the system more transparent, the public organization responsible for the\nelections, the Superior Electoral Court (Tribunal Superior Eleitoral – TSE), published all the data gathered by the EVM in their own website. More precisely, it is possible to get the logs, the digital vote registry (Registro Digital de Voto – RDV), and the voting machine bulletin (Boletim de Urna – BU), of each machine used in the elections.\n\nIn this project I will exctract raw data from the log files and the BUs, transform it, load it into Pandas DataFrames, clean it, analyze it, and then use clustering models to find hidden or non-intuitive patterns and anomalies. Nevertheless, those patterns will be a great finding and should tell a lot about the dataset.\n\nFor that, this notebook focuses on three algorithms: K-Means, DBSCAN and SOMs. They are implemented using mainly two Python libraries: Scikit-Learn and MiniSom.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a id='Data_ingestion'></a>\n## 2. Data Ingestion <a href='#Top' style=\"text-decoration: none;\">^</a>","metadata":{}},{"cell_type":"markdown","source":"### 2.1. Importing the necessary libraries for the ETL process","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport glob, py7zr, os","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2. Extracting the Data","metadata":{"toc-hr-collapsed":true}},{"cell_type":"markdown","source":"#### 2.2.1. Web Scraping\nTo web scrap TSE's website and get the zip files of each EVM, I had do develop a Selenium-based web scraper. This webscraper it able to download all EVMs from every single Federative Unit (also called \"scope\"). Since Brazil has 26 states, 1 Federal District, and 1 scope for the EVMs placed in other countries, the total of scopes / federative units is 28. Therefore, you can run 28 instances of this web scraper to speed up the process of downloading the zip files.","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/ygalvao/BRA_Scraper_2022.git","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python 2022brascraper/web_scraper.py","metadata":{"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2.2. Unzipping\nThe Bash script below automates the process of unzipping the __.bu__ and __.logjez__ files. It will extract the files in the __\"extracted\"__ subfolder.","metadata":{}},{"cell_type":"code","source":"!./extract_bu_and_log.sh","metadata":{"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3. Transforming the Data\n__The BU files (.bu) are in ASN.1 format__, which is not readable by humans nor by Python (at least not natively). Therefore, the Bash script below automates the process of dumping the data from the ASN.1 format files into readable flat files (.txt).\n\nMoreover, the \"bu_etl\" function transforms the data from a BU text file and stores it into a Pandas DataFrame (then returns it).","metadata":{"tags":[]}},{"cell_type":"code","source":"!./dump_bu.sh","metadata":{"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bu_etl(file_path:str)->pd.DataFrame:\n    \"\"\"\n    Transforms the data from a BU text file and stores it into a Pandas DataFrame.\n    \n    Arg.: (str) the path for the BU text file \n    \n    Returns: a Pandas DataFrame.\n    \"\"\"\n    \n    # Read the .txt file\n    with open(file_path) as f:\n        data = f.read()\n\n    # Split the data into lines\n    lines = data.split('\\n')\n\n    # Initialize two empty lists to store the rows of the DataFrame\n    data_dict = {}\n    codes = []\n    brancos_e_nulos = 0\n\n    # Iterate through the lines of the file\n    for i, line in enumerate(lines):\n\n        try:\n            # Split the line by ' = ' and extract the left and right parts\n            left, right = line.split(' = ')\n        except:\n            continue\n\n        # Split the left part by '.', extract the last element (the column name), and delete white spaces before and after it(if there is any)\n        column_name = left.split('.')[-1].strip()\n\n        # Get the desired data and stores it in a dictionary\n        wanted_variables = (\n            'fase',\n            'local',\n            'municipio',\n            'zona',\n            'secao',\n            'qtdEleitoresCompBiometrico',\n            'idEleicao',\n            'qtdEleitoresAptos',\n            'qtdComparecimento',\n            'qtd_votos_13',\n            'qtd_votos_22',\n            'brancos_nulos',\n            'versaoVotacao'\n            )\n\n        value = right.strip(\"'\")\n\n        if column_name == 'codigo':\n            codes.append(value)\n\n        if column_name == 'quantidadeVotos' and len(codes) == 1 and (i > 34 and i < 40):\n            column_name = f'qtd_votos_{codes[0]}'\n            codes.pop()\n\n        if column_name == 'quantidadeVotos' and len(codes) == 1 and (i > 40  and i < 46):\n            column_name = f'qtd_votos_{codes[0]}'\n            codes.pop()\n\n        if column_name == 'quantidadeVotos' and len(codes) == 0 and (i >= 46):\n            column_name = 'brancos_nulos'\n            brancos_e_nulos += int(value)\n            value = brancos_e_nulos\n\n        if column_name in wanted_variables:\n            data_dict[column_name] = [value]\n\n    # Create the DataFrame from the rows\n    df = pd.DataFrame(data_dict)\n    \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4. Loading the Data\nBelow are the procedures (mainly loops) to, finally, load the data into DataFrames that can be used by us, by Scikit-Learn, and by MiniSom.","metadata":{"tags":[]}},{"cell_type":"code","source":"# Importing the data from the BUs into a Pandas DataFrame\nfiles_df2 = glob.glob(\"./BU_e_RDV/extracted/*.txt\")\ndf_bu_list = []\n\n## This loop will \nfor i, file_path in enumerate(files_df2):\n    bu = bu_etl(file_path)\n    df_bu_list.append(bu)\n    os.remove(file_path)\n    \n    if i % 50 == 0:            \n        df_bu = pd.concat(df_bu_list, ignore_index=True)\n        df_bu.to_csv('df_bu.csv') # Saves the DF to a CSV file, so we don't need to run all the ETL process again in the future\n        df_bu_list = [df_bu]\n\nif len(df_bu_list) > 1:\n    df_bu = pd.concat(df_bu_list, ignore_index=True)\n    df_bu.to_csv('df_bu.csv') # Saves the DF to a CSV file, so we don't need to run all the ETL process again in the future","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the data from the log files into a Pandas DataFrame\nfiles_df1 = glob.glob(\"./BU_e_RDV/extracted/*.logjez\") # This is where I stored all the 7Zip logfiles from the \"extract_bu_and_log.sh\" script\ndf_logs_list = []\n\n## This loop will exctract the logs (in flat file format, .dat) and correctly import its data into a Pandas DataFrame\n## At the end of  each iteraction, it will delete the recently extracted .dat file (which has around 700kB)\n## and the original 7Zip file (the .logjez files), in order to save space in disk\nfor i, file_path in enumerate(files_df1):\n    logjez_file = py7zr.SevenZipFile(file_path, mode=\"r\")\n    logjez_file.extractall(path=\"./BU_e_RDV/extracted/\")\n    logjez_file.close()\n    log = pd.read_csv(\n        './BU_e_RDV/extracted/logd.dat',\n        encoding='ISO 8859-1',\n        header=None,\n        names=['date_time', 'event_type', 'id_evm', 'system', 'description', 'authenticator'],\n        sep=None,\n        engine='python'\n    )\n    df_logs_list.append(log)\n    os.remove('./BU_e_RDV/extracted/logd.dat')\n    os.remove(file_path)\n    \n    if i % 50 == 0:\n        df_logs = pd.concat(df_logs_list, ignore_index=True)\n        df_logs.to_csv('df_logs.csv') # Saves the DF to a CSV file, so we don't need to run all the ETL process again in the future\n        df_logs_list = [df_logs]\n        \n    if i > 20000:\n        break\n\nif len(df_logs_list) > 1:\n    df_logs = pd.concat(df_logs_list, ignore_index=True)\n    df_logs.to_csv('df_logs.csv') # Saves the DF to a CSV file, so we don't need to run all the ETL process again in the future","metadata":{"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In case of crash of this notebook (or if you already have the CSV files), \n#you can just read the CSV files that were created in the previous two cells.\n# For that, just run this cell.\n# Nevertheless, running this cell when it is not necessary is safe and harmless.\ndf_logs = pd.read_csv('df_logs.csv', index_col=0)\ndf_bu = pd.read_csv('df_bu.csv', index_col=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_logs.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_bu.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='Exploratory_Data_Analysis'></a>\n## 3. Exploratory Data Analysis <a href='#Top' style=\"text-decoration: none;\">^</a>","metadata":{}},{"cell_type":"markdown","source":"### 3.1. Overall Analysis of the Logs DataFrame","metadata":{"tags":[],"toc-hr-collapsed":true}},{"cell_type":"code","source":"from IPython.display import display as show","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show(df_logs.head(10))\nshow(df_logs.tail(10))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_logs.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that each row in the df_logs DataFrame is one event of one of the systems running inside an EVM.\n\nDue to feasibility purposes, this DataFrame is only a fraction (around 2%) of the data of all the EVM's logs.","metadata":{}},{"cell_type":"markdown","source":"#### 3.1.1. Checking duplicated rows","metadata":{}},{"cell_type":"code","source":"df_logs.duplicated().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1.2. Checking null values","metadata":{}},{"cell_type":"code","source":"df_logs.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1.3. Checking zero values","metadata":{}},{"cell_type":"code","source":"(df_logs == 0).any(axis=1).sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking zero values on the columns\n(df_logs == 0).any(axis=0)","metadata":{"scrolled":true,"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1.4. Checking the possible values for each variable and their frequencies","metadata":{}},{"cell_type":"code","source":"for column in df_logs.columns:\n    print(column+''''s different possible values quantity:''', df_logs[column].nunique(), '\\n')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2. Overall Analysis of the BUs DataFrame","metadata":{"tags":[],"toc-hr-collapsed":true}},{"cell_type":"code","source":"from IPython.display import display as show","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show(df_bu.head(10))\nshow(df_bu.tail(10))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_bu.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that each row in the df_bu DataFrame is one EVM.\n\nDue to feasibility purposes, this DataFrame is only a fraction (around 13%) of the data of all the EVM's logs.","metadata":{}},{"cell_type":"markdown","source":"#### 3.1.1. Checking duplicated rows","metadata":{}},{"cell_type":"code","source":"df_bu.duplicated().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1.2. Checking null values","metadata":{}},{"cell_type":"code","source":"df_bu.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1.3. Checking zero values","metadata":{}},{"cell_type":"code","source":"(df_bu == 0).any(axis=1).sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking zero values on the columns\n(df_bu == 0).any(axis=0)","metadata":{"scrolled":true,"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1.4. Checking the possible values for each variable and their frequencies","metadata":{}},{"cell_type":"code","source":"for column in df_bu.columns:\n    print(column+''''s different possible values quantity:''', df_bu[column].nunique(), '\\n')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_bu.idEleicao.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3. Data Cleaning - Logs DF","metadata":{"toc-hr-collapsed":true}},{"cell_type":"markdown","source":"#### 3.3.1. Deleting unnecessary features","metadata":{"toc-hr-collapsed":true}},{"cell_type":"code","source":"df_logs.drop(columns='authenticator', inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_logs.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.2. Correcting data types","metadata":{"tags":[]}},{"cell_type":"code","source":"df_logs['date_time'] = pd.to_datetime(df_logs.date_time)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in ['event_type', 'id_evm', 'system']:\n    df_logs[column] = df_logs[column].astype('category')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_logs.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.3. Deleting personal data\nThis step is for ethical reasons: it is just to delete some personal data from the poll workers / clerks that are in the log files, like social insurance numbers (Cadastro de Pessoa Física - CPF, in Brazil).","metadata":{"tags":[]}},{"cell_type":"code","source":"df_logs[df_logs.description.str.contains('mesário', case=False)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def delete_cpf(description:str)->str:\n    \"\"\"Deletes only the CPF.\"\"\"\n    \n    new = description.split(' ')\n    for token in new:\n        try:\n            int(token)\n        except:\n            pass\n        else:\n            new.remove(token)\n    \n    new = ' '.join(new)\n    \n    return new","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_logs['description'] = df_logs.description.apply(delete_cpf)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_logs[df_logs.description.str.contains('mesário', case=False)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4. Data Cleaning - BUs DF","metadata":{"toc-hr-collapsed":true}},{"cell_type":"markdown","source":"#### 3.4.1. Deleting unnecessary features","metadata":{"toc-hr-collapsed":true}},{"cell_type":"code","source":"df_bu.drop(columns='fase', inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_bu = df_bu[df_bu.idEleicao=='545'].copy() # The id for the 2nd Round of the 2022 Presidencial Election is 545","metadata":{"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_bu.drop(columns='idEleicao', inplace=True)","metadata":{"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_bu.drop(columns='versaoVotacao', inplace=True)","metadata":{"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_bu.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.4.2. Replacing null values\nAfter analyzing the null values in this DF, I came to the conclusion that they mean, actually, zero (0) values.","metadata":{"tags":[]}},{"cell_type":"code","source":"df_bu.fillna(0, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.4.3. Correcting data types","metadata":{"tags":[]}},{"cell_type":"code","source":"for column in df_bu.columns.to_list():\n    if column[:3] == 'qtd':\n        df_bu[column] = df_bu[column].astype('int')\n    else:\n        df_bu[column] = df_bu[column].astype('category')\n        \ndf_bu['brancos_nulos'] = df_bu['brancos_nulos'].astype('int')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_bu.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5. Descriptive Statistics","metadata":{"toc-hr-collapsed":true}},{"cell_type":"code","source":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport seaborn as sns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a function to classify columns into categorical and quantitative variables\ndef check_variables(df: pd.DataFrame) -> list:\n    \"\"\"\n    Separates the categorical variables from the quantitative variables, and store them in their respective list.\n    \"\"\"\n    \n    cols = df.columns\n    date_cols = df.select_dtypes(include='datetime').columns\n    quantitative_cols = df.select_dtypes(include='number').columns \n    categorical_cols = list(set(cols) - set(quantitative_cols) - set(date_cols))\n    quantitative_cols = set(quantitative_cols) - set(date_cols)\n    \n    return categorical_cols, list(quantitative_cols), list(date_cols)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a function to examine categorical variables\ndef examine_categorical(categ_var : pd.Series, top : int = 10, others : bool = True) -> None:\n    '''\n    This function gets a Pandas DataSeries (categorical column of the Pandas DataFrame) and: \n    - Gets the top 10 (or other chosen quantity) values\n    - Compiles all the other values into \"others\" (or not, if chosen otherwise)\n    - Prints a frequency distribution table\n    - Plots a pie chart\n    - Plots a bar chart\n    '''\n    \n    vc = categ_var.value_counts()\n    vc2 = vc.sort_values(ascending=False)[:top]\n    new_row = pd.Series(data = {'others': vc.sort_values(ascending=False)[top:].sum()})\n    vc3 = pd.concat([vc2, new_row])\n    \n    if others == True:\n        vc = vc3\n        msg = f'''Please, note that, for many reasons, only the top {top} values were considered to these calculations and visualizations.\nAll the other values were compiled into the \"others\" name.'''\n    else:\n        vc = vc2\n        msg = f'''Please, note that, for many reasons, only the top {top} values were considered to these calculations and visualizations.'''\n    \n    # Frequency distribution\n    print(f'''Frequency distribution table for different values of \"{categ_var.name}\" variable: \\n\\n{vc}\\n''')\n    \n    print(msg)\n    \n    \n    # Pie chart\n    vc.plot(\n    kind='pie',\n    ylabel=categ_var.name,\n    autopct='%.2f%%',\n    figsize=(10,10))\n    \n    plt.show()\n    plt.close()\n    \n    # Bar chart\n    bar = vc.plot(\n        kind='bar',\n        figsize=(10,8),\n        align='center')\n    \n    bar.bar_label(bar.containers[0])\n    \n    plt.show()\n    plt.close()\n    \n    print('_' * 120+'\\n' * 3)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a function to examine numerical variables\ndef examine_quant(\n    variable:pd.Series,\n    optmize_n_bins:bool=False,\n    no_outliers:bool=False,\n    n_bins:bool=False\n)->None:\n    '''\n    Gets a Pandas DataSeries and: \n    - Prints measures of central tendancy\n    - Prints measures of spread\n    - Take the outliers out using the 1.5 IQR criterion, if \"no_outliers\" == True\n    - Try to calculate the optimal number of bins for the histogram, if \"optmize_n_bins\" == True\n    - Plots a histogram\n    - Plots box-plot\n    '''\n    \n    var_desc = variable.describe()\n    \n    IQR = var_desc['75%'] - var_desc['25%']\n    \n    print(f'''### Measures for variable '{variable.name}':\n    \n## Measures of center:\nMode: {variable.mode()[0]}\nMean: {var_desc['mean']}\nMedian: {var_desc['50%']}\n\n## Measures of spread:\nMin: {var_desc['min']}\nMax: {var_desc['max']}\nRange: {var_desc['max'] - var_desc['min']}\n\n1st Quartile (Q25): {var_desc['25%']}\n3rd Quartile (Q75): {var_desc['75%']}\nIQR: {IQR}\n\nStandard deviation: {var_desc['std']}\\n''')\n    \n    if no_outliers == True:\n        variable = variable[(variable <= (var_desc['75%'] + 1.5 * IQR)) & (variable >= (var_desc['25%'] - 1.5 * IQR))]\n        \n    def freedman_diaconis(variable : np.ndarray) -> int:\n        \"\"\"\n        Use Freedman Diaconis rule to compute optimal histogram bin width - it tries to return the optimal number of bins. \n        \"\"\"\n\n        data = np.asarray(variable.values, dtype=np.float_)\n        IQR  = stats.iqr(data, rng=(25, 75), scale=1.0, nan_policy='propagate')\n        N    = data.size\n        bw   = (2 * IQR) / np.power(N, 1/3)\n\n        datmin, datmax = data.min(), data.max()\n        datrng = datmax - datmin\n        \n        result = int(((datrng / bw) + 1)/5)\n\n        return result\n    \n    #Histogram\n    if optmize_n_bins:\n        try:\n            n_bins_ = freedman_diaconis(variable)\n        except Exception as e:\n            print(e)\n        else:            \n            variable.hist(bins=n_bins_)\n            plt.show()\n            plt.close()\n    elif n_bins:\n        variable.hist(bins=n_bins)\n        plt.show()\n        plt.close()\n    else:\n        variable.hist()\n        plt.show()\n        plt.close()\n    \n    #Boxplot\n    plt.boxplot(x=variable, labels=[variable.name])\n    plt.ylabel(variable.name)\n    plt.show()\n    plt.close()\n    \n    #Separator line\n    print('_' * 120+'\\n' * 3)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.5.1. Visualizing the Data - Logs DF","metadata":{}},{"cell_type":"code","source":"mpl.rcParams['font.family'] = ['serif']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining better settings for the visualizations using Seaborn\nsns.set(rc={'figure.figsize':(8,6)}, style='whitegrid')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classifying variables by their type\ncat_cols_logs, quan_cols_logs, date_cols_logs = check_variables(df_logs)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Categorical variables:')\nshow(cat_cols_logs)\nprint('\\nQuantitative variables:')\nshow(quan_cols_logs)\nprint('\\nFull date/time variables:')\nshow(date_cols_logs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing and Examining Categorical Variables","metadata":{}},{"cell_type":"code","source":"# Examining categorical variables\nfor variable in cat_cols_logs:\n    examine_categorical(df_logs[variable], top=8, others=True)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no quantitative variables in df_logs.","metadata":{}},{"cell_type":"code","source":"# Examining quantitative variables with outliers\nfor variable in quan_cols_logs:\n    try:\n        examine_quant(df_logs[variable], optmize_n_bins=False, no_outliers=False, n_bins=False)\n    except Exception as e:\n        print(e)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Examining quantitative variables without outliers\nfor variable in quan_cols_logs:\n    try:\n        examine_quant(df_logs[variable], optmize_n_bins=False, no_outliers=True, n_bins=False)\n    except Exception as e:\n        print(e)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.5.2. Visualizing the Data - BUs DF","metadata":{}},{"cell_type":"code","source":"# Classifying variables by their type\ncat_cols_bu, quan_cols_bu, date_cols_bu = check_variables(df_bu)\nprint('Categorical variables:')\nshow(cat_cols_bu)\nprint('\\nQuantitative variables:')\nshow(quan_cols_bu)\nprint('\\nFull date/time variables:')\nshow(date_cols_bu)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing and Examining Categorical Variables","metadata":{}},{"cell_type":"code","source":"# Examining categorical variables\nfor variable in cat_cols_bu:\n    examine_categorical(df_bu[variable], top=12, others=True)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing and Examining Quantitative Variables","metadata":{}},{"cell_type":"code","source":"# Examining quantitative variables with outliers\nfor variable in quan_cols_bu:\n    try:\n        examine_quant(df_bu[variable], optmize_n_bins=False, no_outliers=False, n_bins=False)\n    except Exception as e:\n        print(e)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Examining quantitative variables without outliers\nfor variable in quan_cols_bu:\n    try:\n        examine_quant(df_bu[variable], optmize_n_bins=False, no_outliers=True, n_bins=False)\n    except Exception as e:\n        print(e)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='Clustering'></a>\n## 4. Clustering <a href='#Top' style=\"text-decoration: none;\">^</a>","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Preprocessing the Data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.1.1. Logs DF","metadata":{}},{"cell_type":"markdown","source":"##### Label Encoding","metadata":{}},{"cell_type":"code","source":"labelencoder = LabelEncoder()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_logs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in cat_cols_logs:\n    df_logs[column+'_enc'] = labelencoder.fit_transform(df_logs[column])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_logs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_logs.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_logs = df_logs[['event_type_enc', 'description_enc', 'system_enc', 'id_evm_enc']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_logs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Scaling the data","metadata":{}},{"cell_type":"code","source":"sc = MinMaxScaler(feature_range = (0,1))\nX_logs_scaled = sc.fit_transform(X_logs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_logs_scaled","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_logs.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.1.2. BUs DF","metadata":{}},{"cell_type":"markdown","source":"##### Translating features names to English","metadata":{}},{"cell_type":"code","source":"df_bu.rename(columns={\n    'municipio' : 'municipality',\n    'zona' : 'zone',\n    'secao' : 'section',\n    'qtdEleitoresCompBiometrico' : 'qty_voters_with_biometrics',\n    'qtdEleitoresAptos' : 'qty_voters_able_to_vote',\n    'qtdComparecimento' : 'qty_attendance',\n    'qtd_votos_13' : 'qty_votes_on_13',\n    'qtd_votos_22' : 'qty_votes_on_22',\n    'brancos_nulos' : 'qty_blank_and_null_votes',\n}, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classifying variables by their type again\ncat_cols_bu, quan_cols_bu, date_cols_bu = check_variables(df_bu)\nprint('Categorical variables:')\nshow(cat_cols_bu)\nprint('\\nQuantitative variables:')\nshow(quan_cols_bu)\nprint('\\nFull date/time variables:')\nshow(date_cols_bu)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Label Encoding","metadata":{}},{"cell_type":"code","source":"labelencoder = LabelEncoder()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in cat_cols_bu:\n    df_bu[column+'_enc'] = labelencoder.fit_transform(df_bu[column])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_bu","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_bu.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_bu = df_bu[quan_cols_bu + [column+'_enc' for column in cat_cols_bu]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_bu","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_bu.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_bu_quant = df_bu[quan_cols_bu]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_bu_quant","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Let's create a new feature: attendance_rate\ndf_bu['attendance_rate'] = df_bu.qty_attendance / df_bu.qty_voters_able_to_vote","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's scale it, so it become from 0 to 100, instead of 0 to 1\ndf_bu['attendance_rate'] = df_bu['attendance_rate'] * 100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_bu_quant['attendance_rate'] = df_bu['attendance_rate']","metadata":{"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_bu_quant","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_bu_quant.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='K-Means'></a>\n### 4.2. K-Means","metadata":{}},{"cell_type":"code","source":"from yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import KMeans\nfrom mpl_toolkits.mplot3d import Axes3D","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2.1. Logs DF","metadata":{"toc-hr-collapsed":true}},{"cell_type":"code","source":"model = KMeans(random_state=1, n_init=10)\nvisualizer = KElbowVisualizer(model, k=(2,10))\n\nvisualizer.fit(X_logs_scaled)\nvisualizer.show()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Choosing optimal K\nIn order to find an appropriate number of clusters, the elbow method was used. In this method for this case, the inertia for a number of clusters between 2 and 10 will be calculated. The rule is to choose the number of clusters where you see a kink or \"an elbow\" in the graph.\n\nThe graph above shows the reduction of a distortion score as the number of clusters increases. However, there is no clear \"elbow\" visible. The underlying algorithm suggests 4 clusters. A choice of 4 or 5 clusters seems to be fair.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"##### Building the model and getting the clusters","metadata":{"tags":[]}},{"cell_type":"code","source":"KM_5_clusters_logs = KMeans(n_clusters=5, init='k-means++', random_state=123, n_init=10).fit(X_logs_scaled) # initialise and fit K-Means model\n\nKM_5_clustered_logs = X_logs.copy()\nKM_5_clustered_logs.loc[:,'Cluster'] = KM_5_clusters_logs.labels_ # append labels to points","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KM_5_clustered_logs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing the clusters","metadata":{}},{"cell_type":"code","source":"fig1, (axes) = plt.subplots(1,2,figsize=(12,5))\n\n\nscat_1 = sns.scatterplot(\n    KM_5_clustered_logs,\n    x='event_type_enc',\n    y='system_enc',\n    hue='Cluster',\n    ax=axes[0],\n    palette='Set1',\n    legend='full'\n)\n\nsns.scatterplot(\n    KM_5_clustered_logs,\n    x='id_evm_enc',\n    y='event_type_enc',\n    hue='Cluster',\n    palette='Set1',\n    ax=axes[1],\n    legend='full')\n\naxes[0].scatter(KM_5_clusters_logs.cluster_centers_[:,1],KM_5_clusters_logs.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\naxes[1].scatter(KM_5_clusters_logs.cluster_centers_[:,0],KM_5_clusters_logs.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig1, (axes) = plt.subplots(1,2,figsize=(12,5))\n\n\nscat_1 = sns.scatterplot(\n    KM_5_clustered_logs,\n    x='description_enc',\n    y='id_evm_enc',\n    hue='Cluster',\n    ax=axes[0],\n    palette='Set1',\n    legend='full'\n)\n\nsns.scatterplot(\n    KM_5_clustered_logs,\n    x='description_enc',\n    y='event_type_enc',\n    hue='Cluster',\n    palette='Set1',\n    ax=axes[1],\n    legend='full')\n\naxes[0].scatter(KM_5_clusters_logs.cluster_centers_[:,1],KM_5_clusters_logs.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\naxes[1].scatter(KM_5_clusters_logs.cluster_centers_[:,0],KM_5_clusters_logs.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Checking the size of the clusters","metadata":{}},{"cell_type":"code","source":"KM4_clust_sizes = KM4_clustered.groupby('Cluster').size().to_frame()\nKM4_clust_sizes.columns = [\"KM_size\"]\nKM4_clust_sizes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2.2. BUs DF - with labeled categorical features","metadata":{"toc-hr-collapsed":true}},{"cell_type":"code","source":"model = KMeans(random_state=1, n_init=10)\nvisualizer = KElbowVisualizer(model, k=(2,10))\n\nvisualizer.fit(X_bu)\nvisualizer.show()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Choosing optimal K\nIn order to find an appropriate number of clusters, the elbow method was used. In this method for this case, the inertia for a number of clusters between 2 and 10 will be calculated. The rule is to choose the number of clusters where you see a kink or \"an elbow\" in the graph.\n\nThe graph above shows the reduction of a distortion score as the number of clusters increases. However, there is no clear \"elbow\" visible. The underlying algorithm suggests 4 clusters. A choice of 4 or 5 clusters seems to be fair.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"##### Building the model and getting the clusters","metadata":{"tags":[]}},{"cell_type":"code","source":"KM_5_clusters = KMeans(n_clusters=5, init='k-means++', random_state=123, n_init=10).fit(X_bu) # initialise and fit K-Means model\n\nKM5_clustered = X_bu.copy()\nKM5_clustered.loc[:,'Cluster'] = KM_5_clusters.labels_ # append labels to points","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KM5_clustered","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing the clusters","metadata":{}},{"cell_type":"code","source":"fig1, (axes) = plt.subplots(1,2,figsize=(12,5))\n\nscat_1 = sns.scatterplot(\n    KM5_clustered,\n    x='municipality_enc',\n    y='qty_votes_on_22',\n    hue='Cluster',\n    ax=axes[0],\n    palette='Set1',\n    legend='full'\n)\n\nsns.scatterplot(\n    KM5_clustered,\n    x='municipality_enc',\n    y='qty_votes_on_13',\n    hue='Cluster',\n    palette='Set1',\n    ax=axes[1],\n    legend='full')\n\naxes[0].scatter(KM_5_clusters.cluster_centers_[:,1],KM_5_clusters.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\naxes[1].scatter(KM_5_clusters.cluster_centers_[:,0],KM_5_clusters.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig1, (axes) = plt.subplots(1,2,figsize=(12,5))\n\nscat_1 = sns.scatterplot(\n    KM5_clustered,\n    x='qty_voters_with_biometrics',\n    y='qty_votes_on_22',\n    hue='Cluster',\n    ax=axes[0],\n    palette='Set1',\n    legend='full'\n)\n\nsns.scatterplot(\n    KM5_clustered,\n    x='qty_voters_with_biometrics',\n    y='qty_votes_on_13',\n    hue='Cluster',\n    palette='Set1',\n    ax=axes[1],\n    legend='full')\n\naxes[0].scatter(KM_5_clusters.cluster_centers_[:,1],KM_5_clusters.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\naxes[1].scatter(KM_5_clusters.cluster_centers_[:,0],KM_5_clusters.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig1, (axes) = plt.subplots(1,2,figsize=(12,5))\n\nscat_1 = sns.scatterplot(\n    KM5_clustered,\n    x='qty_voters_with_biometrics',\n    y='qty_blank_and_null_votes',\n    hue='Cluster',\n    ax=axes[0],\n    palette='Set1',\n    legend='full'\n)\n\nsns.scatterplot(\n    KM5_clustered,\n    x='qty_blank_and_null_votes',\n    y='qty_votes_on_22',\n    hue='Cluster',\n    palette='Set1',\n    ax=axes[1],\n    legend='full')\n\naxes[0].scatter(KM_5_clusters.cluster_centers_[:,1],KM_5_clusters.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\naxes[1].scatter(KM_5_clusters.cluster_centers_[:,0],KM_5_clusters.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Checking the size of the clusters","metadata":{}},{"cell_type":"code","source":"KM5_clust_sizes = KM5_clustered.groupby('Cluster').size().to_frame()\nKM5_clust_sizes.columns = [\"KM_size\"]\nKM5_clust_sizes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2.2. BUs DF - without categorical features","metadata":{"toc-hr-collapsed":true}},{"cell_type":"code","source":"model = KMeans(random_state=1, n_init=10)\nvisualizer = KElbowVisualizer(model, k=(2,10))\n\nvisualizer.fit(X_bu_quant)\nvisualizer.show()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Choosing optimal K\nIn order to find an appropriate number of clusters, the elbow method was used. In this method for this case, the inertia for a number of clusters between 2 and 10 will be calculated. The rule is to choose the number of clusters where you see a kink or \"an elbow\" in the graph.\n\nThe graph above shows the reduction of a distortion score as the number of clusters increases. However, there is no clear \"elbow\" visible. The underlying algorithm suggests 4 clusters. A choice of 4 or 5 clusters seems to be fair.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"##### Building the model and getting the clusters","metadata":{"tags":[]}},{"cell_type":"code","source":"KM_5_clusters_only_quant = KMeans(n_clusters=5, init='k-means++', random_state=123, n_init=10).fit(X_bu_quant) # initialise and fit K-Means model\n\nKM5_clustered_only_quant = X_bu_quant.copy()\nKM5_clustered_only_quant.loc[:,'Cluster'] = KM_5_clusters_only_quant.labels_ # append labels to points","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KM5_clustered_only_quant","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing the clusters","metadata":{}},{"cell_type":"code","source":"fig1, (axes) = plt.subplots(1,2,figsize=(12,5))\n\nscat_1 = sns.scatterplot(\n    KM5_clustered_only_quant,\n    x='qty_votes_on_13',\n    y='qty_votes_on_22',\n    hue='Cluster',\n    ax=axes[0],\n    palette='Set1',\n    legend='full'\n)\n\nsns.scatterplot(\n    KM5_clustered_only_quant,\n    x='qty_blank_and_null_votes',\n    y='qty_votes_on_13',\n    hue='Cluster',\n    palette='Set1',\n    ax=axes[1],\n    legend='full')\n\n#axes[0].scatter(KM_5_clusters_only_quant.cluster_centers_[:,1],KM_5_clusters_only_quant.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\n#axes[1].scatter(KM_5_clusters_only_quant.cluster_centers_[:,0],KM_5_clusters_only_quant.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig1, (axes) = plt.subplots(1,2,figsize=(12,5))\n\nscat_1 = sns.scatterplot(\n    KM5_clustered_only_quant,\n    x='qty_voters_with_biometrics',\n    y='qty_votes_on_22',\n    hue='Cluster',\n    ax=axes[0],\n    palette='Set1',\n    legend='full'\n)\n\nsns.scatterplot(\n    KM5_clustered_only_quant,\n    x='qty_voters_with_biometrics',\n    y='qty_votes_on_13',\n    hue='Cluster',\n    palette='Set1',\n    ax=axes[1],\n    legend='full')\n\naxes[0].scatter(KM_5_clusters_only_quant.cluster_centers_[:,1],KM_5_clusters_only_quant.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\naxes[1].scatter(KM_5_clusters_only_quant.cluster_centers_[:,0],KM_5_clusters_only_quant.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig1, (axes) = plt.subplots(1,2,figsize=(12,5))\n\nscat_1 = sns.scatterplot(\n    KM5_clustered_only_quant,\n    x='qty_voters_with_biometrics',\n    y='qty_blank_and_null_votes',\n    hue='Cluster',\n    ax=axes[0],\n    palette='Set1',\n    legend='full'\n)\n\nsns.scatterplot(\n    KM5_clustered_only_quant,\n    x='qty_voters_with_biometrics',\n    y='qty_attendance',\n    hue='Cluster',\n    palette='Set1',\n    ax=axes[1],\n    legend='full')\n\n#axes[0].scatter(KM_5_clusters_only_quant.cluster_centers_[:,1],KM_5_clusters_only_quant.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\naxes[1].scatter(KM_5_clusters_only_quant.cluster_centers_[:,0],KM_5_clusters_only_quant.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig1, (axes) = plt.subplots(1,2,figsize=(12,5))\n\nsns.scatterplot(\n    KM5_clustered_only_quant,\n    x='qty_voters_able_to_vote',\n    y='qty_blank_and_null_votes',\n    hue='Cluster',\n    ax=axes[0],\n    palette='Set1',\n    legend='full'\n)\n\nsns.scatterplot(\n    KM5_clustered_only_quant,\n    x='qty_voters_able_to_vote',\n    y='qty_attendance',\n    hue='Cluster',\n    palette='Set1',\n    ax=axes[1],\n    legend='full')\n\n#axes[0].scatter(KM_5_clusters_only_quant.cluster_centers_[:,1],KM_5_clusters_only_quant.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\n#axes[1].scatter(KM_5_clusters_only_quant.cluster_centers_[:,0],KM_5_clusters_only_quant.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig1, (axes) = plt.subplots(1,2,figsize=(12,5))\n\nsns.scatterplot(\n    KM5_clustered_only_quant,\n    x='attendance_rate',\n    y='qty_votes_on_13',\n    hue='Cluster',\n    ax=axes[0],\n    palette='Set1',\n    legend='full'\n)\n\nsns.scatterplot(\n    KM5_clustered_only_quant,\n    x='attendance_rate',\n    y='qty_votes_on_22',\n    hue='Cluster',\n    palette='Set1',\n    ax=axes[1],\n    legend='full')\n\n#axes[0].scatter(KM_5_clusters_only_quant.cluster_centers_[:,1],KM_5_clusters_only_quant.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\n#axes[1].scatter(KM_5_clusters_only_quant.cluster_centers_[:,0],KM_5_clusters_only_quant.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig1, (axes) = plt.subplots(1,2,figsize=(12,5))\n\nsns.scatterplot(\n    KM5_clustered_only_quant,\n    x='qty_voters_with_biometrics',\n    y='attendance_rate',\n    hue='Cluster',\n    ax=axes[0],\n    palette='Set1',\n    legend='full'\n)\n\nsns.scatterplot(\n    KM5_clustered_only_quant,\n    x='qty_voters_able_to_vote',\n    y='attendance_rate',\n    hue='Cluster',\n    palette='Set1',\n    ax=axes[1],\n    legend='full')\n\n#axes[0].scatter(KM_5_clusters_only_quant.cluster_centers_[:,1],KM_5_clusters_only_quant.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\n#axes[1].scatter(KM_5_clusters_only_quant.cluster_centers_[:,0],KM_5_clusters_only_quant.cluster_centers_[:,2], marker='s', s=40, c=\"blue\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Checking the size of the clusters","metadata":{}},{"cell_type":"code","source":"KM5_clust_sizes = KM5_clustered_only_quant.groupby('Cluster').size().to_frame()\nKM5_clust_sizes.columns = [\"KM_size\"]\nKM5_clust_sizes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='DBSCAN'></a>\n### 4.3. DBSCAN\nIn DBSCAN there are two major hyperparameters:\n\n- eps\n- min_samples\n\nIt is difficult arbitrarily to say what values will work the best. Therefore, I will first create a matrix of combinations.","metadata":{"tags":[]}},{"cell_type":"code","source":"from sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom itertools import product","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.3.1. Logs DF","metadata":{"toc-hr-collapsed":true}},{"cell_type":"markdown","source":"After many tries, I realized that DBSCAN will not work with the Logs DF, since it always crashes the Jupyter Lab (regardless if it is in my own computer or in Google Colab or in a powerful machine in Google Cloud Platform Workbench).\n\nThis is probably due to the fact that this dataset is a huge one (it has more than 2,000,000 rows, and it is already much smaller), and also it has only categorical variables (even though they are already encoded).","metadata":{}},{"cell_type":"markdown","source":"#### 4.3.1. BUs DF","metadata":{"toc-hr-collapsed":true}},{"cell_type":"markdown","source":"##### Chosing optimal parameters","metadata":{}},{"cell_type":"code","source":"eps_values = np.arange(6,10.75,0.75) # eps values to be investigated\nmin_samples = np.arange(6,12) # min_samples values to be investigated\n\nDBSCAN_params = list(product(eps_values, min_samples))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Because DBSCAN creates clusters itself based on those two parameters let's check the number of generated clusters.\n\nno_of_clusters = []\nsil_score = []\n\nfor p in DBSCAN_params:\n    DBS_clustering = DBSCAN(eps=p[0], min_samples=p[1]).fit(X_bu_quant)\n    no_of_clusters.append(len(np.unique(DBS_clustering.labels_)))\n    sil_score.append(silhouette_score(X_bu_quant, DBS_clustering.labels_))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples'])   \ntmp['No_of_clusters'] = no_of_clusters\n\npivot_1 = pd.pivot_table(tmp, values='No_of_clusters', index='Min_samples', columns='Eps')\n\nfig, ax = plt.subplots(figsize=(12,6))\nsns.heatmap(pivot_1, annot=True,annot_kws={\"size\": 16}, cmap=\"YlGnBu\", ax=ax)\nax.set_title('Number of clusters')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The heatplot immediately above shows that, with the given parameters, the number of clusters vary from 2 to 330. However, most of the combinations gives more than 20 clusters. Nevertheless, we can safely choose numbers located on the bottom-left or the bottom-right corner of the heatmap.","metadata":{}},{"cell_type":"markdown","source":"##### Cluster # 1","metadata":{}},{"cell_type":"markdown","source":"##### Building the model and getting the clusters","metadata":{}},{"cell_type":"code","source":"DBS_clustering = DBSCAN(eps=16, min_samples=14).fit(X_bu_quant)\n\nDBSCAN_clustered = X_bu_quant.copy()\nDBSCAN_clustered.loc[:,'Cluster'] = DBS_clustering.labels_ # append labels to points","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Checking the size of the clusters","metadata":{}},{"cell_type":"code","source":"DBSCAN_clust_sizes = DBSCAN_clustered.groupby('Cluster').size().to_frame()\nDBSCAN_clust_sizes.columns = [\"DBSCAN_size\"]\nDBSCAN_clust_sizes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DBSCAN created 8 clusters plus outliers cluster (-1). Sizes of clusters vary significantly. There are 6579 outliers.","metadata":{}},{"cell_type":"markdown","source":"##### Cluster # 2","metadata":{}},{"cell_type":"markdown","source":"##### Building the model and getting the clusters","metadata":{}},{"cell_type":"code","source":"DBS_clustering = DBSCAN(eps=6, min_samples=10).fit(X_bu_quant)\n\nDBSCAN_clustered = X_bu_quant.copy()\nDBSCAN_clustered.loc[:,'Cluster'] = DBS_clustering.labels_ # append labels to points","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Checking the size of the clusters","metadata":{}},{"cell_type":"code","source":"DBSCAN_clust_sizes = DBSCAN_clustered.groupby('Cluster').size().to_frame()\nDBSCAN_clust_sizes.columns = [\"DBSCAN_size\"]\nDBSCAN_clust_sizes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DBSCAN created 3 clusters plus outliers cluster (-1). Sizes of clusters are almost the same. There are 34613 outliers in this cluster.","metadata":{}},{"cell_type":"markdown","source":"<a id='SOM'></a>\n### 4.3. SOM\nSelf Organizing Map (SOM) is an unsupervised ANN that uses competitive learning to update its weights - i.e Competition, Cooperation and Adaptation.\n\nEach neuron of the output layer is present with a vector with dimension n. The distance between each neuron present at the output layer and the input data is computed. The neuron with the lowest distance is termed as the most suitable fit.\n\nUpdating the vector of the suitable neuron in the final process is known as adaptation, along with its neighbour in cooperation. After selecting the suitable neuron and its neighbours, we process the neuron to update. The more the distance between the neuron and the input, the more the data grows. ","metadata":{"tags":[]}},{"cell_type":"code","source":"from minisom import MiniSom ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.3.1. BUs DF","metadata":{"toc-hr-collapsed":true}},{"cell_type":"code","source":"X_bu_quant.values.shape # Let's just check the number of columns in the dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Setup # 1","metadata":{}},{"cell_type":"markdown","source":"##### Building the model","metadata":{}},{"cell_type":"code","source":"neurons_a = 15\nneurons_b = 15\nsom = MiniSom(neurons_a, neurons_b, X_bu_quant.values.shape[1], random_seed=0, learning_rate=.1, sigma=1.5)\nsom.pca_weights_init(X_bu_quant.values)\nsom.train(X_bu_quant.values, 10000, verbose=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing the U-Matrix","metadata":{}},{"cell_type":"markdown","source":"The U-Matrix is a common way to visualize the results of a Self-Organizing Map (SOM). It is a 2D representation of the SOM's neurons and the distances between them, where each cell in the U-Matrix corresponds to a neuron in the SOM. The color of each cell represents the distance between that neuron and its neighbors.\n\nEach cell in the U-Matrix corresponds to a neuron in the SOM, and the numbers inside the cells are the indexes of the neurons. The lines separating the cells represent the distances between the neurons.\n\nIn general, cells with similar colors in the U-Matrix tend to have similar input vectors assigned to them. This means that these neurons form clusters of similar data in the input space. The darker the color, the more similar are the vectors assigned to that neuron and its neighbors, indicating that these neurons are closer in the input space.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nfrequencies = som.activation_response(X_bu_quant.values)\nplt.pcolor(frequencies.T, cmap='Blues')\nplt.colorbar()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Setup # 2","metadata":{}},{"cell_type":"markdown","source":"##### Building the model","metadata":{}},{"cell_type":"code","source":"neurons_a = 15\nneurons_b = 15\nsom = MiniSom(neurons_a, neurons_b, X_bu_quant.values.shape[1], random_seed=0, learning_rate=.2, sigma=12)\nsom.pca_weights_init(X_bu_quant.values)\nsom.train(X_bu_quant.values, 10000, verbose=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing the U-Matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nfrequencies = som.activation_response(X_bu_quant.values)\nplt.pcolor(frequencies.T, cmap='Blues')\nplt.colorbar()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Setup # 3","metadata":{}},{"cell_type":"markdown","source":"##### Building the model","metadata":{}},{"cell_type":"code","source":"neurons_a = 15\nneurons_b = 15\nsom = MiniSom(neurons_a, neurons_b, X_bu_quant.values.shape[1], random_seed=0, learning_rate=.5, sigma=1)\nsom.pca_weights_init(X_bu_quant.values)\nsom.train(X_bu_quant.values, 10000, verbose=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing the U-Matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nfrequencies = som.activation_response(X_bu_quant.values)\nplt.pcolor(frequencies.T, cmap='Blues')\nplt.colorbar()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Setup # 4","metadata":{}},{"cell_type":"markdown","source":"##### Building the model","metadata":{}},{"cell_type":"code","source":"neurons_a = 15\nneurons_b = 15\nsom = MiniSom(neurons_a, neurons_b, X_bu_quant.values.shape[1], random_seed=0, learning_rate=.75, sigma=1)\nsom.pca_weights_init(X_bu_quant.values)\nsom.train(X_bu_quant.values, 10000, verbose=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing the U-Matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nfrequencies = som.activation_response(X_bu_quant.values)\nplt.pcolor(frequencies.T, cmap='Blues')\nplt.colorbar()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Setup # 5","metadata":{}},{"cell_type":"markdown","source":"##### Building the model","metadata":{}},{"cell_type":"code","source":"neurons_a = 15\nneurons_b = 15\nsom = MiniSom(neurons_a, neurons_b, X_bu_quant.values.shape[1], random_seed=0, learning_rate=.25, sigma=1)\nsom.pca_weights_init(X_bu_quant.values)\nsom.train(X_bu_quant.values, 50000, verbose=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing the U-Matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nfrequencies = som.activation_response(X_bu_quant.values)\nplt.pcolor(frequencies.T, cmap='Blues')\nplt.colorbar()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Setup # 6","metadata":{}},{"cell_type":"markdown","source":"##### Building the model","metadata":{}},{"cell_type":"code","source":"neurons_a = 15\nneurons_b = 15\nsom = MiniSom(neurons_a, neurons_b, X_bu_quant.values.shape[1], random_seed=0, learning_rate=.25, sigma=1)\nsom.pca_weights_init(X_bu_quant.values)\nsom.train(X_bu_quant.values, 150000, verbose=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing the U-Matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nfrequencies = som.activation_response(X_bu_quant.values)\nplt.pcolor(frequencies.T, cmap='Blues')\nplt.colorbar()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Setup # 6","metadata":{}},{"cell_type":"markdown","source":"##### Building the model","metadata":{}},{"cell_type":"code","source":"neurons_a = 25\nneurons_b = 25\nsom = MiniSom(neurons_a, neurons_b, X_bu_quant.values.shape[1], random_seed=0, learning_rate=.25, sigma=1)\nsom.pca_weights_init(X_bu_quant.values)\nsom.train(X_bu_quant.values, 200000, verbose=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing the U-Matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nfrequencies = som.activation_response(X_bu_quant.values)\nplt.pcolor(frequencies.T, cmap='Blues')\nplt.colorbar()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Setup # 6","metadata":{}},{"cell_type":"markdown","source":"##### Building the model","metadata":{}},{"cell_type":"code","source":"neurons_a = 40\nneurons_b = 40\nsom = MiniSom(neurons_a, neurons_b, X_bu_quant.values.shape[1], random_seed=0, learning_rate=.25, sigma=1)\nsom.pca_weights_init(X_bu_quant.values)\nsom.train(X_bu_quant.values, 500000, verbose=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing the U-Matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nfrequencies = som.activation_response(X_bu_quant.values)\nplt.pcolor(frequencies.T, cmap='Blues')\nplt.colorbar()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Setup # 7","metadata":{}},{"cell_type":"markdown","source":"##### Building the model","metadata":{}},{"cell_type":"code","source":"neurons_a = 35\nneurons_b = 35\nsom = MiniSom(neurons_a, neurons_b, X_bu_quant.values.shape[1], random_seed=0, learning_rate=.35, sigma=1)\nsom.pca_weights_init(X_bu_quant.values)\nsom.train(X_bu_quant.values, 500000, verbose=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing the U-Matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nfrequencies = som.activation_response(X_bu_quant.values)\nplt.pcolor(frequencies.T, cmap='Blues')\nplt.colorbar()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Setup # 8","metadata":{}},{"cell_type":"markdown","source":"##### Building the model","metadata":{}},{"cell_type":"code","source":"neurons_a = 10\nneurons_b = 10\nsom = MiniSom(neurons_a, neurons_b, X_bu_quant.values.shape[1], random_seed=0, learning_rate=.35, sigma=1)\nsom.pca_weights_init(X_bu_quant.values)\nsom.train(X_bu_quant.values, 50000, verbose=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing the U-Matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nfrequencies = som.activation_response(X_bu_quant.values)\nplt.pcolor(frequencies.T, cmap='Blues')\nplt.colorbar()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Generating Clusters","metadata":{}},{"cell_type":"markdown","source":"Of course, we can also generate clusters from SOMs. These clusters are based on the winner neurons (basically, each winner neuron represents a cluster).","metadata":{}},{"cell_type":"code","source":"# each winner neuron represents a cluster\nwinner_coordinates = np.array([som.winner(x) for x in X_bu_quant.values]).T\n# with np.ravel_multi_index we convert the bidimensional\n# coordinates to a monodimensional index\ncluster_index = np.ravel_multi_index(winner_coordinates, (neurons_a, neurons_b))\n\n# plotting the clusters using the first 2 dimentions of the data\nfor c in np.unique(cluster_index):\n    plt.scatter(X_bu_quant.values[cluster_index == c, 0],\n                X_bu_quant.values[cluster_index == c, 1], label='cluster='+str(c), alpha=.7)\n\n# plotting centroids\n#for centroid in som.get_weights():\n#    plt.scatter(centroid[:, 0], centroid[:, 1], marker='x', \n#                s=80, linewidths=35, color='k', label='centroid')\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Setup # 9","metadata":{}},{"cell_type":"markdown","source":"##### Building the model","metadata":{}},{"cell_type":"code","source":"neurons_a = 3\nneurons_b = 5\nsom = MiniSom(neurons_a, neurons_b, X_bu_quant.values.shape[1], random_seed=0, learning_rate=.35, sigma=.5)\nsom.pca_weights_init(X_bu_quant.values)\nsom.train(X_bu_quant.values, 50000, verbose=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Visualizing the U-Matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nfrequencies = som.activation_response(X_bu_quant.values)\nplt.pcolor(frequencies.T, cmap='Blues')\nplt.colorbar()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Generating Clusters","metadata":{}},{"cell_type":"code","source":"# each winner neuron represents a cluster\nwinner_coordinates = np.array([som.winner(x) for x in X_bu_quant.values]).T\n# with np.ravel_multi_index we convert the bidimensional\n# coordinates to a monodimensional index\ncluster_index = np.ravel_multi_index(winner_coordinates, (neurons_a, neurons_b))\n\n# plotting the clusters using the first 2 dimentions of the data\nfor c in np.unique(cluster_index):\n    plt.scatter(X_bu_quant.values[cluster_index == c, 0],\n                X_bu_quant.values[cluster_index == c, 1], label='cluster='+str(c), alpha=.7)\n\n# plotting centroids\n#for centroid in som.get_weights():\n#    plt.scatter(centroid[:, 0], centroid[:, 1], marker='x', \n#                s=80, linewidths=35, color='k', label='centroid')\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Setup # 9","metadata":{}},{"cell_type":"markdown","source":"##### Building the model","metadata":{}},{"cell_type":"code","source":"neurons_a = 2\nneurons_b = 3\nsom = MiniSom(neurons_a, neurons_b, X_bu_quant.values.shape[1], random_seed=0, learning_rate=.3, sigma=.5)\nsom.pca_weights_init(X_bu_quant.values)\nsom.train(X_bu_quant.values, 50000, verbose=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Generating Clusters","metadata":{}},{"cell_type":"code","source":"# Defining a function to make it easier and faster\ndef generate_cluster_from_som(dim1:int, dim2:int)->None:\n    # each winner neuron represents a cluster\n    winner_coordinates = np.array([som.winner(x) for x in X_bu_quant.values]).T\n    # with np.ravel_multi_index we convert the bidimensional\n    # coordinates to a monodimensional index\n    cluster_index = np.ravel_multi_index(winner_coordinates, (neurons_a, neurons_b))\n\n    # plotting the clusters using the first 2 dimentions of the data\n    for c in np.unique(cluster_index):\n        plt.scatter(X_bu_quant.values[cluster_index == c, dim1],\n                    X_bu_quant.values[cluster_index == c, dim2], label='cluster='+str(c), alpha=.7)\n\n    plt.xlabel(X_bu_quant.columns.to_list()[dim1])\n    plt.ylabel(X_bu_quant.columns.to_list()[dim2])\n    # plotting centroids\n    #for centroid in som.get_weights():\n    #    plt.scatter(centroid[:, 0], centroid[:, 1], marker='x', \n    #                s=80, linewidths=35, color='k', label='centroid')\n    plt.legend()\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_cluster_from_som(0, 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_cluster_from_som(2, 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_cluster_from_som(2, 3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_cluster_from_som(6, 3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_cluster_from_som(6, 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_cluster_from_som(4, 6)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_cluster_from_som(2, 6)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='Conclusions'></a>\n## 5. Conclusions <a href='#Top' style=\"text-decoration: none;\">^</a>","metadata":{}},{"cell_type":"markdown","source":"### 5.1. Patterns Found in the Data","metadata":{}},{"cell_type":"markdown","source":"K-Means and SOM algorithms and models generated some interesting clusters and, although it is possible to explain clusters from different points of view (basing on different variables), there are some visualizations in which the clusters appear more and are much more readable.\n\nBasically, K-means generated the following 5 clusters from the logs DF:\n- Red (0): EVMs with medium quantity of votes on 22 (Bolsonaro) and medium to high quantity of votes on 13 (Lula). This cluster also shows EVMs with high quantity of voters with biometrics and evms with a high attendance rate.\n\n- Blue (1): EVMs with low quantity of votes on 22 (Bolsonaro) and low quantity of votes on 13 (Lula). This cluster also shows EVMs with low quantity of voters with biometrics.\n\n- Green (2): EVMs with medium quantity of votes on 22 (Bolsonaro) and low quantity of votes on 13 (Lula).\n\n- Purple (3): EVMs with high quantity of votes on 22 (Bolsonaro) and medium quantity of votes on 13 (Lula).\n\n- Orange (4): EVMs with low quantity of votes on 22 (Bolsonaro) and high quantity of votes on 13 (Lula). This cluster also shows EVMs with medium quantity of voters with biometrics.\n\nAnd SOM generated the following 6 clusters from the logs DF:\n- Dark Blue (0): EVMs with medium quantity of votes on 22 (Bolsonaro), medium to high quantity of votes on 13 (Lula), high attendance rate, and high quantity of voters with biometrics.\n\n- Green (1): EVMs with low quantity of votes on 22 (Bolsonaro), high quantity of votes on 13 (Lula), high attendance rate, and medium quantity of voters with biometrics.\n\n- Red (2): EVMs with low quantity of votes on 22 (Bolsonaro), medium quantity of votes on 13 (Lula), medium to high attendance rate, and medium quantity of voters with biometrics.\n\n- Purple (3): EVMs with low quantity of votes on 22 (Bolsonaro), low (the lowest) quantity of votes on 13 (Lula), medium to low attendance rate (the lowest), and low quantity of voters with biometrics.\n\n- Yellow (4): EVMs with high (the highest) quantity of votes on 22 (Bolsonaro), low quantity of votes on 13 (Lula), medium to high attendance rate, and medium quantity of voters with biometrics.\n\n- Light Blue (5): EVMs with low quantity of votes on 22 (Bolsonaro), low quantity of votes on 13 (Lula), medium to high attendance rate, and low quantity of voters with biometrics (the lowest).","metadata":{}},{"cell_type":"markdown","source":"### 5.2. Algorithms Comparison","metadata":{}},{"cell_type":"markdown","source":"Regarding the three clustering algorithms used in this notebook, we can say that K-Means is the easiest to use, whereas SOM is the fastest and also seems to be the most complete and most customizable one - coincidence or not (probably not), SOM is the only algorithm that is considered to be within the Deep Learning subset, since it is an Artificial Neural Network.\n\nDBSCAN is very heavy and needs a lot of computing power and tweaking to be really effective. Moreover, DBSCAN seems to be more suitable for anomaly detection than for clustering itself although, unfortunately, I wasn't able to effectively detect anomalies using DBSCAN in this project due to the reasons I will still mention in these conclusions.","metadata":{}},{"cell_type":"markdown","source":"### 5.3. Problems and Challenges","metadata":{}},{"cell_type":"markdown","source":"The first major challenge in this project was the data ingestion phase, because it needed a really efficient web scraper (in which I invested a lot of time to develop it) and because of the huge volume of data: in total, there were used around 490,000 EVMs in the 2nd round of the Brazilian Federal Elections last year. I ended up downloading the zip files of around 110,000 EVMs, and I couldn't keep going because of the limitations of my computer and of my Google Cloud Platform account (yes, I had to use my GCP Vertex AI Workbench to this project, since my computer wasn't enough).\n\nThe second major challeng was, also, a problem and was related to the first one: the lack of computing power to properly process this huge amount of data. Even using cloud resources, I got many crashes and I had to decrease the size of the datasets (mainly the logs one) to keep working in this project. Trying to work with the whole data from all the logs and all the EVM's boulletins (_Boletins de Urna_ - BUs) from 110,000 EVMs wasn't feasible at all.\n\nThe third and last problem was the type of the data of the logs. The fact that all the features of the logs were categorical didn't help in anything. In fact, it was the major reason why the K-Means clusters of the logs data didn't seem useful at all. I tried to solve this problem by label encoding it (since one-hot / dummy encoding wasn't feasible) and scaling it, but it didn't helped too much.","metadata":{}},{"cell_type":"markdown","source":"<a id='Conclusions'></a>\n## 6. References <a href='#Top' style=\"text-decoration: none;\">^</a>","metadata":{}},{"cell_type":"markdown","source":"<a href='https://github.com/JustGlowing/minisom' style=\"text-decoration: none;\">[1]</a> \tVictor Dey, \"Beginners Guide to Self-Organizing Maps\", AIM, 2021<br>\n<a href='https://analyticsindiamag.com/beginners-guide-to-self-organizing-maps/' style=\"text-decoration: none;\">[2]</a> \tGiuseppe Vettigli, \"MiniSom\", GitHUb, 2022<br>\n<a href='https://https://www.kaggle.com/code/datark1/customers-clustering-k-means-dbscan-and-ap' style=\"text-decoration: none;\">[3]</a> \tRobert Kwiatkowski, \"Customers clustering: K-Means, DBSCAN and AP\", Kaggle, 2022<br>\n<a href='https://towardsdatascience.com/clustering-on-numerical-and-categorical-features-6e0ebcf1cbad' style=\"text-decoration: none;\">[4]</a> \tJorge Martín Lasaosa, \"Clustering on numerical and categorical features\", Towards Data Science, 2021<br>","metadata":{}}]}